# Skeleton for IBM datathon detector fine tuner
# Will use BERT text classification model from hugging face and fine tune it

# Tokenise data into a form BERT understands
def tokenize_function(examples):
    pass

# define metrics to be used by the transformer
def compute_metrics(eval_pred):
    pass

# The main function
def main():
    pass

if __name__ == "__main__":
    main()